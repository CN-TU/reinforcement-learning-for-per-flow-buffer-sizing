\documentclass[10pt,sigconf,letterpaper,anonymous]{acmart}

\usepackage[english]{babel}
\usepackage{blindtext}

%Conference Info
\acmYear{2020}
\copyrightyear{2020}
%\setcopyright{acmcopyright}
\acmConference{CoNEXT '20}{December 1-4, 2020}{Barcelona, Spain}
%\acmPrice{TBA}
%\acmDOI{TBA}
%\acmISBN{TBA}

\usepackage[nomain, toc, acronym]{glossaries}

\newacronym{dl}{DL}{Deep Learning}
\newacronym{rl}{RL}{Reinforcement Learning}
\newacronym{ours}{OurSolution}{OurSolution}



\begin{document}
\title{On-line Learning of Custom Per-flow Queuing Policies with Deep Reinforcement Learning}

%\subtitle{Paper \# XXX, XXX pages}
% \author{Firstname Lastname}
% \authornote{Note}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{Affiliation}
%   \streetaddress{Address}
%   \city{City} 
%   \state{State} 
%   \postcode{Zipcode}
% }
% \email{email@domain.com}


\begin{abstract}
The increasing number of different, incompatible congestion control algorithms has led to an increased deployment of fair queuing. Fair queuing isolates each network flow and gives it a separate queue. It can thus guarantee fairness for each flow even if the flows' congestion controls are not inherently fair. So far, each queue in the fair queuing system either has a fixed, static maximum size or is managed by an Active Queue Management algorithm like CoDel. In this paper we design and implement an AQM mechanism that dynamically learns the optimal buffer size for each flow with respect to a specified utility function. We show that our Deep Learning based algorithm can correctly fingerprint each flow and assign the optimal queue size. Besides that we also show that our mechanism can even learn completely on-line and can continuously adapt. Finally we demonstrate that the computational overhead of our approach is sufficiently low to allow for deployment on regular hardware. 
\end{abstract}

\maketitle

\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}

\end{document}
